{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import os, sys, re\n",
    "from collections import defaultdict\n",
    "#import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "#%matplotlib nbagg\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "Write a Python program (OSX or Linux preferred) that queries to the top 500 Alexa domains/sites:\n",
    "- http://www.alexa.com/topsites\n",
    "- https://www.dropbox.com/s/pqsimknj77ywqbn/top-1m.csv.tar.gz?dl=0\n",
    "\n",
    "### CDN analysis\n",
    "\n",
    "- Filter the domains served by a CDN (e.g., Akamai)\n",
    "- Rank CDN providers by number of sites\n",
    "- Calculate the average response time for the index page of each site (aka Time To First Byte) per CDN provider, and rank them by speed (separate DNS resolution, TCP connection, SSL negotiation and receive time ideally);\n",
    "\n",
    "### BGP analysis\n",
    "\n",
    "- Determine the ASN that each of the 500 sites maps to (based on hosting IP address)\n",
    "- Rank ASNs by number of sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCDF(data):\n",
    "    xdata = np.sort(data)\n",
    "    ydata = [i/len(xdata) for i in range(len(xdata))]\n",
    "    return xdata, ydata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDN Analysis\n",
    "\n",
    "## Load Data\n",
    "load top 500 alexa websites from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sites = pd.read_csv('top-1m.csv', nrows=500, header = None, names = ['rank', 'url'])\n",
    "sites = pd.read_csv('top-1m-new.csv', nrows=500, header = None, names = ['rank', 'url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get CDN for website\n",
    "- using whois on IP address (via nslookup for domain)\n",
    "- extract fields from DNS headers\n",
    "- checking IP from multiple server locations\n",
    "- downloading all objects on the website homepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia.org'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site = sites['url'][4]\n",
    "site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get IP address using host -t a <site\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIP(site):\n",
    "    # use host command to get IP for site (domain url netloc)\n",
    "    # use timeout of 5.0 s for sites that are firewalled and can't be looked up\n",
    "    try:\n",
    "        out = subprocess.check_output(['host', '-t', 'a', site], stderr=subprocess.STDOUT, timeout=5.0).decode('UTF-8')\n",
    "        # set site IP as first IP address returned by host command - using split is more efficient than regex\n",
    "        site_IP = out.split('\\n')[0].split(\" \")[-1]\n",
    "    except:\n",
    "        print('host '+site+' process ran too long')\n",
    "        site_IP = ''\n",
    "    return site_IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'172.217.167.46'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getIP('google.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host pornhub.com process ran too long\n",
      "host ok.ru process ran too long\n",
      "host livejasmin.com process ran too long\n",
      "host xvideos.com process ran too long\n",
      "host xhamster.com process ran too long\n",
      "host xnxx.com process ran too long\n",
      "host chaturbate.com process ran too long\n",
      "host yts.am process ran too long\n",
      "host youporn.com process ran too long\n",
      "host 1337x.to process ran too long\n",
      "host redtube.com process ran too long\n",
      "host rutracker.org process ran too long\n",
      "host sex.com process ran too long\n"
     ]
    }
   ],
   "source": [
    "sites['IP_addr'] = sites['url'].apply(getIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensure IP is valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipaddress\n",
    "\n",
    "def validateIP(site_IP):\n",
    "    try:\n",
    "        ipaddress.ip_address(site_IP)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites['isValidIP'] = sites['IP_addr'].apply(validateIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find organization and email servers using whois <IPADDR\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findOrg(site_IP):\n",
    "    # use whois <IPADDR> to find and parse the organization and email server of site_IP\n",
    "    # return dict {Org, OrgName, Email}\n",
    "    \n",
    "    Org = []\n",
    "    OrgName = []\n",
    "    EmailServer = []\n",
    "    exclude_orgs = ['AFRINIC','APNIC','ARIN','LACNIC','IANA']\n",
    "    \n",
    "    try:\n",
    "        out2 = subprocess.check_output(['whois', site_IP], stderr=subprocess.STDOUT, timeout=5.0).decode('UTF-8')\n",
    "\n",
    "        for line in out2.split('\\n'):\n",
    "            lower_line = line.lower()\n",
    "\n",
    "            if 'organisation:' in lower_line or 'org:' in lower_line or 'organization:' in lower_line:\n",
    "                if not any([excl_org in line for excl_org in exclude_orgs]):\n",
    "                    Org.append(line.split(':')[1].strip())\n",
    "\n",
    "            if 'org-name:' in lower_line or 'orgname:' in lower_line:\n",
    "                OrgName.append(line.split(':')[1].strip())\n",
    "\n",
    "            if 'email:' in lower_line or 'e-mail:' in lower_line:\n",
    "                EmailServer.append(line.split('@')[1].strip())\n",
    "                #print(line)\n",
    "    \n",
    "    except:\n",
    "        print('whois '+site_IP+' process error/ran too long')\n",
    "        \n",
    "    #output = {'Org': list(set(Org))[0],\n",
    "    #        'OrgName': list(set(OrgName))[0],\n",
    "    #        'EmailServer': list(set(EmailServer))[0] }\n",
    "    output = {'Org': list(set(Org)),\n",
    "            'OrgName': list(set(OrgName)),\n",
    "            'EmailServer': list(set(EmailServer)) }\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Org': ['Google LLC (GOGL)', 'Level 3 Parent, LLC (LPL-141)'],\n",
       " 'OrgName': ['Google LLC', 'Level 3 Parent, LLC'],\n",
       " 'EmailServer': ['google.com', 'level3.com']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findOrg('8.8.8.8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separate valid IP websites and find organizations for these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_valid = sites[ sites['isValidIP'] ]\n",
    "#sites_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whois 186.192.90.5 process error/ran too long\n",
      "whois 212.129.33.171 process error/ran too long\n",
      "whois 82.165.229.87 process error/ran too long\n"
     ]
    }
   ],
   "source": [
    "sites_org = sites_valid['IP_addr'].apply(lambda s: pd.Series( findOrg(s) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_cdn = sites_valid.merge(sites_org, left_index=True, right_index=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sites_cdn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check organization, orgname, email against list of well known CDN names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdn names from wikipedia, www.cdnlist.com\n",
    "CDN_names = sorted( list( pd.read_csv('CDNnames.csv', sep = '\\n', header = None, names = ['CDN'])['CDN'].unique() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank number of websites per CDN\n",
    "- sort dataframe\n",
    "- bar chart\n",
    "\n",
    "## Request Index page\n",
    "- time to first byte\n",
    "- time to DNS resolution\n",
    "- time to TCP connection\n",
    "- time to SSL negotiation\n",
    "- receive time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

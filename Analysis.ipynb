{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "%pylab inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA\n",
    "1. Get host IP and ASN for Alexa top 500 websites: use socket and pyasn (offline routeviews) libraries\n",
    "- Find CDN of each website: use list of common CDNs and their related domains\n",
    "    - Compare site to well known CDN domains (such as googleusercontent.com, cloudfront.net, etc.)\n",
    "    - Parse homepage \n",
    "        - Parsing static objects on homepage to find number of resources per well known CDN domain\n",
    "    - Parse whois\n",
    "        - 'Organization' field might have the CDN name\n",
    "        - site contact email servers used might be related to certain CDN domains\n",
    "- Use curl to get timings for accessing sites: save response time for at least 20 requests per site\n",
    "    - -v and --trace-time flags allow us to read timing information in real time\n",
    "    - curl -w flag allows us to write time since the request was issued for name lookup, connection, SSL negotiation, and data reception\n",
    "    - Calculate times:\n",
    "        - t_dns = time for DNS resolution (no redirects) = time_namelookup - time_redirect  \n",
    "        - t_tcp = time for TCP connection (SYN/SYNACK) = time_connect - time_namelookup  \n",
    "        - t_ssl = time for SSL handshake (only if https) = time_appconnect - time_connect  \n",
    "        - t_fbyte = time_starttransfer\n",
    "        - t_wait = time between issuing GET request and first byte received = time_starttransfer - time_pretransfer  \n",
    "        - t_rx = time to receive data from first to last byte = time_total - time_starttransfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sites = pd.read_csv('top-1m-new.csv', nrows=500, header = None, names = ['rank', 'site'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get IP and ASNs\n",
    "- Get IP using socket.gethostbyname method\n",
    "- Get ASN using pyasn (can also be done using whois, downloading routeviews data, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "def getIP(s):\n",
    "    try:\n",
    "        IP = socket.gethostbyname(s)\n",
    "        # ISP blocked domains return IP 49.207.46.6, 49.207.46.24, 49.207.46.34\n",
    "        if IP in ['49.207.46.6', '49.207.46.24', '49.207.46.34']:\n",
    "            print(\"Blocked site \"+s)\n",
    "            return False\n",
    "        else:\n",
    "            return IP\n",
    "    except:\n",
    "        print(\"Error accessing site \"+s)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked site pornhub.com\n",
      "Blocked site ok.ru\n",
      "Blocked site livejasmin.com\n",
      "Blocked site xvideos.com\n",
      "Error accessing site googleusercontent.com\n",
      "Blocked site xhamster.com\n",
      "Error accessing site exosrv.com\n",
      "Blocked site xnxx.com\n",
      "Blocked site chaturbate.com\n",
      "Blocked site yts.am\n",
      "Blocked site youporn.com\n",
      "Blocked site 1337x.to\n",
      "Error accessing site cloudfront.net\n",
      "Blocked site redtube.com\n",
      "Blocked site rutracker.org\n",
      "Error accessing site banvenez.com\n",
      "Error accessing site bp.blogspot.com\n",
      "Error accessing site exdynsrv.com\n",
      "Blocked site sex.com\n",
      "Error accessing site wixsite.com\n"
     ]
    }
   ],
   "source": [
    "df_sites['IP'] = df_sites['site'].apply(getIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15169, '8.8.8.0/24')\n"
     ]
    }
   ],
   "source": [
    "import pyasn\n",
    "\n",
    "asndb = pyasn.pyasn('output/ipasn_20181212.dat')  #downloaded pyasn_util_download.py --latest\n",
    "print(asndb.lookup('8.8.8.8'))\n",
    "\n",
    "def findASN(ip):\n",
    "    if ip:\n",
    "        return asndb.lookup(ip)[0]\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sites['ASN'] = df_sites['IP'].apply(findASN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>site</th>\n",
       "      <th>IP</th>\n",
       "      <th>ASN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>google.com</td>\n",
       "      <td>172.217.167.46</td>\n",
       "      <td>15169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>172.217.166.238</td>\n",
       "      <td>15169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>facebook.com</td>\n",
       "      <td>157.240.13.35</td>\n",
       "      <td>32934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>baidu.com</td>\n",
       "      <td>123.125.115.110</td>\n",
       "      <td>4808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>103.102.166.224</td>\n",
       "      <td>14907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank           site               IP    ASN\n",
       "0     1     google.com   172.217.167.46  15169\n",
       "1     2    youtube.com  172.217.166.238  15169\n",
       "2     3   facebook.com    157.240.13.35  32934\n",
       "3     4      baidu.com  123.125.115.110   4808\n",
       "4     5  wikipedia.org  103.102.166.224  14907"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sites.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate unblocked IPs and sites for analysis\n",
    "- 20 of 500 sites are unreachable or blocked by the ISP\n",
    "- Querying these sites and urls returns an IP at the edge of the ISP we're connected to\n",
    "- Most of these sites are porn related or known for adware/malware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid sites for further analysis: 480\n",
      "\n",
      "List of blocked sites: ['pornhub.com', 'ok.ru', 'livejasmin.com', 'xvideos.com', 'googleusercontent.com', 'xhamster.com', 'exosrv.com', 'xnxx.com', 'chaturbate.com', 'yts.am', 'youporn.com', '1337x.to', 'cloudfront.net', 'redtube.com', 'rutracker.org', 'banvenez.com', 'bp.blogspot.com', 'exdynsrv.com', 'sex.com', 'wixsite.com']\n"
     ]
    }
   ],
   "source": [
    "df_valid = df_sites[df_sites['IP'] != False]\n",
    "\n",
    "print(\"Number of valid sites for further analysis: %s\\n\" % (len(df_valid)))\n",
    "\n",
    "print(\"List of blocked sites: %s\" % list(df_sites[df_sites['IP'] == False]['site']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save site_to_IP dictionary for valid sites\n",
    "\n",
    "site_to_IP = (df_valid.set_index('site')['IP']).to_dict()\n",
    "json.dump(site_to_IP, open('output/site_to_IP.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find CDN\n",
    "- CDNnames\n",
    "- CDNdomains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CDNdomains\n",
    "\n",
    "with open('CDNnames.csv', 'r') as f:\n",
    "    CDNnames = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### page parse data\n",
    "- saved in output/homepage/\n",
    "    - use requests to download website and save it\n",
    "- count number of objects per url\n",
    "    - load downloaded page and use BeautifulSoup to parse it\n",
    "    - count sources and links for a, link, img, script tags on page\n",
    "    - static objects like images and scripts are usually on website host IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib\n",
    " \n",
    "def getnetloc(loc):\n",
    "        if loc is None or loc is \"\":  # empty src or link instead of local src or link\n",
    "            return None\n",
    "        return urllib.parse.urlparse(loc).netloc\n",
    "\n",
    "def count_netlocs(data):\n",
    "    soup = BeautifulSoup(data)\n",
    "    \n",
    "    netlocs_static = []\n",
    "    #netlocs_all = []\n",
    "\n",
    "    # static\n",
    "    tags = soup.findAll('script')\n",
    "    for tag in tags:\n",
    "        loc = tag.get('src')\n",
    "        netlocs_static.append( getnetloc(loc) )\n",
    "\n",
    "    tags = soup.findAll('img')\n",
    "    for tag in tags:\n",
    "        loc = tag.get('src')  # srcset should be counted only once for the src\n",
    "        netlocs_static.append( getnetloc(loc) )\n",
    "\n",
    "    tags = soup.findAll('a')\n",
    "    for tag in tags:\n",
    "        loc = tag.get('href')\n",
    "        netlocs_static.append( getnetloc(loc) )\n",
    "    \n",
    "    \"\"\"\n",
    "    # all\n",
    "    tags = soup.findAll('link')\n",
    "    for tag in tags:\n",
    "        loc = tag.get('href')\n",
    "        netlocs_all.append( getnetloc(loc)\n",
    "\n",
    "    tags = soup.findAll('meta')\n",
    "    for tag in tags:\n",
    "        loc = tag.get('content')\n",
    "        netlocs_all.append( getnetloc(loc) )\n",
    "    \"\"\"  \n",
    "    return Counter(netlocs_static)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "sites_list = list(sites['site'])\n",
    "counter = {}\n",
    "bad_site = []\n",
    "\n",
    "for site in sites_list:\n",
    "    \n",
    "    file = 'output/homepage/'+site\n",
    "    if os.path.exists(file):\n",
    "        with open(file, 'r') as f:\n",
    "            data = f.read()\n",
    "            cnt = count_netlocs( data )\n",
    "            del cnt[None]  # remove empty src NOT local src\n",
    "            counter[site] = cnt.most_common()  # sort Counter\n",
    "    else:\n",
    "        bad_site.append(site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### whois data\n",
    "- saved in output/whoissite/ and output/whoisIP/\n",
    "- sometimes Organization names contains the name of the registry or andminitrator. This should be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def loadwhoisIP(site):\n",
    "    file = 'output/whoisIP/'+site\n",
    "    if os.path.exists(file):\n",
    "        f = open(file, 'r')\n",
    "        whoisIP = f.read()\n",
    "        f.close()\n",
    "        return whoisIP\n",
    "    else:\n",
    "        print(\"Please perform whois for site %s\" %site)\n",
    "        return None\n",
    "\n",
    "def loadwhoissite(site):\n",
    "    file = 'output/whoissite/'+site\n",
    "    if os.path.exists(file):\n",
    "        f = open(file, 'r')\n",
    "        whoissite = f.read()\n",
    "        f.close()\n",
    "        return whoissite\n",
    "    else:\n",
    "        print(\"Please perform whois for site %s\" %site)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search orgs for both whois IP and whois site\n",
    "# search email only for whois IP\n",
    "\n",
    "def searchOrg(whoisdata):\n",
    "    \n",
    "    Org = []\n",
    "    OrgKeywords = ['organisation:', 'org:', 'organization:', 'org-name:', 'orgname:']\n",
    "    exclude_orgs = [e.lower() for e in ['AFRINIC','APNIC','ARIN','LACNIC','IANA',\n",
    "                'Asia Pacific Network Information Centre', 'Administered by RIPE NCC', 'RIPE NCC', \n",
    "                                    'Registration Association', 'VeriSign Global Registry Services'] ]\n",
    "    if whoisdata is not None:\n",
    "        for line in whoisdata.split('\\n'):\n",
    "            lower_line = line.lower()\n",
    "\n",
    "            for keyword in OrgKeywords:\n",
    "                if keyword in lower_line:\n",
    "                    if not any([excl_org in lower_line for excl_org in exclude_orgs]):\n",
    "                        org = line.split(':')[1].strip()\n",
    "                        if not org in Org:\n",
    "                            Org.append(org)\n",
    "    return Org\n",
    "\n",
    "def searchEmail(whoisdata):\n",
    "    Email = []\n",
    "    EmailKeywords = ['@', 'email:', 'mailbox:', 'e-mail:']\n",
    "    #exclude_emails = [e.lower() for e in ['verisign-grs.com', 'verisigninc.com', 'Registrar'] ]\n",
    "\n",
    "    if whoisdata is not None:\n",
    "        for line in whoisdata.split('\\n'):\n",
    "            lower_line = line.lower()\n",
    "\n",
    "            for keyword in EmailKeywords:\n",
    "                if keyword in lower_line:\n",
    "                    email = line.split('@')[1].strip()\n",
    "                    if not email in Email:\n",
    "                        Email.append(email)\n",
    "                    \n",
    "    return Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site_to_IP = df_valid.set_index('site')['IP'].to_dict()\n",
    "\n",
    "# after parsing whois info from CDN_BGP_Analysis\n",
    "#sites_org = pd.read_pickle('output/df_org_email_info.pkl')\n",
    "\n",
    "#sites_org = df_valid['IP'].apply(lambda s: pd.Series( findOrg(s) ) )\n",
    "#sites_cdn = df_valid.merge(sites_org, left_index=True, right_index=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate CDN\n",
    "- compare site to known CDN domains\n",
    "- compare most popular url that hosts objects on site homepage to known CDN domains\n",
    "- compare organization name from whois to CDN names\n",
    "- compare email address from whois to CDN domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareDomain(site):\n",
    "    # TODO domain vs CDNdomains.(dict)\n",
    "    return cdn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareURL(url_counter):\n",
    "    # TODO url: count vs CDNdomains.(dict)\n",
    "    return cdn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### whois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareOrg(org_list):\n",
    "    # TODO if cdnname in org_list\n",
    "    return cdn\n",
    "\n",
    "def compareEmail(email_list):\n",
    "    # TODO email vs cdndomain\n",
    "    return cdn\n",
    "\n",
    "\n",
    "def matchCDN(org_list):\n",
    "    if len(org_list)>0:\n",
    "        for org in org_list:\n",
    "            org_lower = org.lower()\n",
    "            for cdn in CDN_names:\n",
    "                cdn_lower = cdn.lower()\n",
    "                if (cdn_lower in org_lower):\n",
    "                    return cdn\n",
    "                \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'site_to_IP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-617c10a2d734>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIP\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msite_to_IP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mwhois1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadwhoisIP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mOrg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearchOrg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhois1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mEmail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearchEmail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhois1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'site_to_IP' is not defined"
     ]
    }
   ],
   "source": [
    "for site,IP in site_to_IP.items():\n",
    "    \n",
    "    cdn = compareDomain(site)\n",
    "    if cdn is None:\n",
    "        url_counter = \n",
    "        cdn = compareURL()\n",
    "    \n",
    "    whois1 = loadwhoisIP(site)\n",
    "    Org1 = searchOrg(whois1)\n",
    "    Email = searchEmail(whois1)\n",
    "    \n",
    "    whois2 = loadwhoissite(site)\n",
    "    Org2 = searchOrg(whois2)\n",
    "    \n",
    "    #print(site, IP, Org1, Org2, Email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get curl request timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
